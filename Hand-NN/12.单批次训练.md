
```python
import copy

import numpy as np

import init_data as cp

NETWORK_SHAPE = [2, 3, 4, 5, 2]
BATCH_SIZE = 5
LEARNING_RATE = 0.01


def precise_loss_function(predicted, real):
    real_matrix = np.zeros((len(real), NETWORK_SHAPE[-1]))
    real_matrix[:, 1] = real
    real_matrix[:, 0] = 1 - real
    product = np.sum(predicted * real_matrix, axis=1)
    return 1 - product


def get_final_layer_preAct_demands(predicted_values, target_vector):
    target = np.zeros((len(target_vector), 2))
    target[:, 1] = target_vector
    target[:, 0] = 1 - target_vector

    for i in range(len(target_vector)):
        if np.dot(target[i], predicted_values[i]) > 0.5:
            target[i] = np.array([0, 0])
        else:
            target[i] = (target[i] - 0.5) * 2
    return target


def normalize(array):
    max_number = np.max(np.absolute(array), axis=1, keepdims=True)
    scale_rate = np.where(max_number == 0, 1, (1 / max_number))
    norm = array * scale_rate
    return norm


def vector_normalize(array):
    max_number = np.max(np.absolute(array))
    scale_rate = np.where(max_number == 0, 1, (1 / max_number))
    norm = array * scale_rate
    return norm


def classify(probabilities):
    classification = np.rint(probabilities[:, 1])
    return classification


def activation_softmax(inputs):
    max_values = np.max(inputs, axis=1, keepdims=True)
    slided_inputs = inputs - max_values
    exp_values = np.exp(slided_inputs)
    norm_base = np.sum(exp_values, axis=1, keepdims=True)
    norm_values = exp_values / norm_base
    return norm_values


def activation_ReLu(inputs):
    return np.maximum(0, inputs)


# 因为这个矩阵在后面，所以几行几列就是几个输入，几个输出
def create_weights(n_inputs, n_neurons):
    return np.random.randn(n_inputs, n_neurons)


def create_biases(n_neurons):
    return np.random.randn(n_neurons)


class Layer():
    def __init__(self, n_inputs, n_neurons):
        self.weights = np.random.randn(n_inputs, n_neurons)
        self.biases = np.random.randn(n_neurons)

    def layer_forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases
        return self.output

    def layer_backward(self, preWeights_values, aftWeights_demands):
        # print(f"aftWeights_demands{aftWeights_demands}*self.weights.T{self.weights.T}={np.dot(aftWeights_demands, self.weights.T)}")
        preWeights_demands = np.dot(aftWeights_demands, self.weights.T)  # 求出的是改变w的△w,不管最后节点有多少，有多少套w，每个w都按照这个变！

        condition = (preWeights_values > 0)
        # print(f"preWeights_values是{preWeights_values}")
        # print(f"condition是{condition}")
        value_derivatives = np.where(condition, 1, 0)  # 把output[-2]大于0给1，小于0给0形成虚拟output[-2]
        # print(f"value_derivatives是{value_derivatives}")
        preActs_demands = value_derivatives * preWeights_demands  # 这个虚拟output[-2]乘刚刚的△w就是再前向的demands
        # print(f"value_derivatives{value_derivatives} * preWeights_demands{preWeights_demands}是{value_derivatives * preWeights_demands}")
        norm_preActs_demands = normalize(preActs_demands)

        get_weight_adjust_matrix = self.get_weight_adjust_matrix(preWeights_values, aftWeights_demands)
        norm_weight_adjust_matrix = normalize(get_weight_adjust_matrix)
        return (norm_preActs_demands, norm_weight_adjust_matrix)

    def get_weight_adjust_matrix(self, preWeights_values, aftWeights_demands):
        plain_weights = np.full(self.weights.shape, 1)
        weights_adjust_matrix = np.full(self.weights.shape, 0.0)
        plain_weights_T = plain_weights.T

        for i in range(BATCH_SIZE):
            a = (plain_weights_T * preWeights_values[i, :]).T * aftWeights_demands[i, :]
            weights_adjust_matrix += a
        weights_adjust_matrix = weights_adjust_matrix / BATCH_SIZE
        return weights_adjust_matrix
    # BATCH_SIZE<输入数据的个数
    # 不明白为什么是层的输入乘需求函数而不是当前层的权重乘需求函数。
    # 应该是让原先输入值如果大了就小一点然后赋予给权重矩阵，或者因为AB=C，所以降低哪个都是一个意思？


class Network():
    def __init__(self, network_shape):
        self.shape = network_shape
        self.n_layers = len(network_shape) - 1
        self.layers = []
        for i in range(self.n_layers):
            layer = Layer(network_shape[i], network_shape[i + 1])
            self.layers.append(layer)

    def network_forward(self, inputs):
        outputs = [inputs]
        for i in range(self.n_layers):
            layer_sum = self.layers[i].layer_forward(outputs[i])
            if i != self.n_layers - 1:
                layer_output = activation_ReLu(layer_sum)
                layer_output = normalize(layer_sum)
            else:
                layer_output = activation_softmax(layer_sum)
            outputs.append(layer_output)

        return outputs

    def network_backward(self, layeroutputs, target_vector):
        backup_network = copy.deepcopy(self)  # 备用网络
        # 我们要把这些矩阵和偏置值更新成新的，但是不知道是不是更准确！
        # 把备用网络前馈再跑一遍，如果更好就代替。
        # 用两个变量指向同一个参数，当一个变量修改时，另一个变量也会修改
        preAct_demands = get_final_layer_preAct_demands(layeroutputs[-1], target_vector)
        for i in range(self.n_layers):
            layer = backup_network.layers[self.n_layers - (1 + i)]  # 倒序
            # 修正最后一层的bias，会产生过拟合！
            if i != 0:  # 不是最后一层
                layer.biases += LEARNING_RATE * np.mean(preAct_demands, axis=0)
                layer.biases = vector_normalize(layer.biases)  # 一维

            outputs = layeroutputs[len(layeroutputs) - (1 + i) - 1]
            results_list = layer.layer_backward(outputs, preAct_demands)
            preAct_demands = results_list[0]  # 需求值重新更新
            weights_adjust_maxtrix = results_list[1]
            layer.weights += LEARNING_RATE * weights_adjust_maxtrix
            layer.weights = normalize(layer.weights)  # 二维

        return backup_network

    def one_batch_train(self, batch):
        inputs = batch[:, (0, 1)]
        targets = copy.deepcopy(batch[:, 2]).astype(int)
        outputs = self.network_forward(inputs)
        precise_loss = precise_loss_function(outputs[-1], targets)

        if np.mean(precise_loss) <= 0.1:  # 猜的
            print("No need for training")
        else:
            backup_network = self.network_backward(outputs, targets)
            backup_outputs = backup_network.network_forward(inputs)
            backup_precise_loss = precise_loss_function(backup_outputs[-1], targets)

            if np.mean(precise_loss) >= np.mean(backup_precise_loss):
                for i in range(self.n_layers - 1):
                    self.layers[i].weights = backup_network.layers[i].weights.copy()
                    self.layers[i].biases = backup_network.layers[i].biases.copy()
                print("Improved")
            else:
                print("No improved")
        print("-------------------------------------------------------------")




def main():
    data = cp.create_data(BATCH_SIZE)
    cp.plot_data(data, 'original classification')
    print(data)

    network = Network(NETWORK_SHAPE)  # 建立
    network.one_batch_train(data)



    # outputs = network.network_forward(inputs)
    # classification = classify(outputs[-1])
    # print(classification)  # 老网络的分类结果
    # data[:, 2] = classification
    # cp.plot_data(data, 'Before training')
    # # loss = precise_loss_function(outputs[-1], targets)
    # # print(loss)
    # # demand = get_final_layer_preAct_demands(outputs[-1], targets)
    # # print(demand)
    # #
    # # adjust_matrix = network.layers[-1].get_weight_adjust_matrix(outputs[-2], demand)
    # # print("aaaawwwww", adjust_matrix)
    # #
    # # # 测试层反向传播
    # # layer_backwards = network.layers[-1].layer_backward(outputs[-2], demand)
    # # print(layer_backwards)
    #
    # # 测试备用网络
    # backup_network = network.network_backward(outputs, targets)
    # new_outputs = backup_network.network_forward(inputs)
    # new_classification = classify(new_outputs[-1])
    # data[:, 2] = new_classification
    # cp.plot_data(data, 'After training')


main()



```

单批次训练，但是更多的是No improved
我们后面介绍多批次的训练！

