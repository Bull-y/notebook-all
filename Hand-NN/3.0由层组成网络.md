
```python
import numpy as np


def activation_ReLu(inputs):
    return np.maximum(0, inputs)


# 因为这个矩阵在后面，所以几行几列就是几个输入，几个输出
def create_weights(n_inputs, n_neurons):
    return np.random.randn(n_inputs, n_neurons)


def create_biases(n_neurons):
    return np.random.randn(n_neurons)


class Layer():
    def __init__(self, n_inputs, n_neurons):
        self.weights = np.random.randn(n_inputs, n_neurons)
        self.biases = np.random.randn(n_neurons)

    def layer_forward(self, inputs):
        sum1 = np.dot(inputs, self.weights) + self.biases
        output = activation_ReLu(sum1)
        return output


a11 = 0.9
a12 = -0.4

a21 = -0.8
a22 = 0.5

a31 = -0.5
a32 = 0.8

a41 = 0.7
a42 = -0.3

a51 = -0.9
a52 = 0.4
# 5行2列，2是自拟定的，5行是五个神经元所以5个数据，一个数据有2个像素，行块表示一个数据，一共是一个batch
inputs = np.array([[a11, a12],
                   [a21, a22],
                   [a31, a32],
                   [a41, a42],
                   [a51, a52]])
# 2个输入3个输出，因为一层有3个神经元，故生成35,3
layer1 = Layer(2, 3)
layer2 = Layer(3, 4)
layer3 = Layer(4, 2)

output1 = layer1.layer_forward(inputs)
output2 = layer2.layer_forward(output1)
output3 = layer3.layer_forward(output2)

print(output3)



```

每个层都还要一行，但是100层还是需要100行？！