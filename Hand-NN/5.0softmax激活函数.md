## 随机生成权重偏差矩阵
```python
import numpy as np

NETWORK_SHAPE = [2, 3, 4, 2]


def activation_softmax(inputs):
    max_values = np.max(inputs, axis=1, keepdims=True)
    slided_inputs = inputs - max_values
    exp_values = np.exp(slided_inputs)
    norm_base = np.sum(exp_values, axis=1, keepdims=True)
    norm_values = exp_values / norm_base
    return norm_values


def activation_ReLu(inputs):
    return np.maximum(0, inputs)


# 因为这个矩阵在后面，所以几行几列就是几个输入，几个输出
def create_weights(n_inputs, n_neurons):
    return np.random.randn(n_inputs, n_neurons)


def create_biases(n_neurons):
    return np.random.randn(n_neurons)


class Layer():
    def __init__(self, n_inputs, n_neurons):
        self.weights = np.random.randn(n_inputs, n_neurons)
        self.biases = np.random.randn(n_neurons)

    def layer_forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases
        return self.output


class Network():
    def __init__(self, network_shape):
        self.shape = network_shape
        self.n_layers = len(network_shape)-1
        self.layers = []
        for i in range(self.n_layers):
            layer = Layer(network_shape[i], network_shape[i + 1])
            self.layers.append(layer)

    def network_forward(self, inputs):
        outputs = [inputs]
        for i in range(self.n_layers):
            layer_sum = self.layers[i].layer_forward(outputs[i])
            if i != self.n_layers-1:
                layer_output = activation_ReLu(layer_sum)
            else:
                layer_output = activation_softmax(layer_sum)
            outputs.append(layer_output)
        return outputs


a11 = 0.9
a12 = -0.4

a21 = -0.8
a22 = 0.5

a31 = -0.5
a32 = 0.8

a41 = 0.7
a42 = -0.3

a51 = -0.9
a52 = 0.4
# 5行2列，2是自拟定的，5行是五个神经元所以5个数据，一个数据有2个像素，行块表示一个数据，一共是一个batch
inputs = np.array([[a11, a12],
                   [a21, a22],
                   [a31, a32],
                   [a41, a42],
                   [a51, a52]])


def main():
    network = Network(NETWORK_SHAPE)  # 建立
    print(network.network_forward(inputs))  # 前馈
    # activation_softmax(inputs)


main()




```
生成数据，不要手敲啦~！