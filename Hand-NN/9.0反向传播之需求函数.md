
```python
import numpy as np

import init_data as cp
import copy
NETWORK_SHAPE = [2, 3, 4, 2]


def precise_loss_function(predicted, real):
    real_matrix = np.zeros((len(real), 2))
    real_matrix[:, 1] = real
    real_matrix[:, 0] = 1 - real
    product = np.sum(predicted * real_matrix, axis=1)
    return 1 - product


def get_final_layer_preAct_demands(predicted_values, target_vector):
    target = np.zeros((len(target_vector), 2))
    target[:,1] = target_vector
    target[:,0] = 1 - target_vector

    for i in range(len(target_vector)):
        if np.dot(target[i], predicted_values[i]) > 0.5:
            target[i] = np.array([0, 0])
        else:
            target[i] = (target[i] - 0.5) * 2
    return target


def normalize(array):
    max_number = np.max(np.absolute(array), axis=1, keepdims=True)
    scale_rate = np.where(max_number == 0, 0, 1 / max_number)
    norm = array * scale_rate
    return norm


def classify(probabilities):
    classification = np.rint(probabilities[:, 1])
    return classification


def activation_softmax(inputs):
    max_values = np.max(inputs, axis=1, keepdims=True)
    slided_inputs = inputs - max_values
    exp_values = np.exp(slided_inputs)
    norm_base = np.sum(exp_values, axis=1, keepdims=True)
    norm_values = exp_values / norm_base
    return norm_values


def activation_ReLu(inputs):
    return np.maximum(0, inputs)


# 因为这个矩阵在后面，所以几行几列就是几个输入，几个输出
def create_weights(n_inputs, n_neurons):
    return np.random.randn(n_inputs, n_neurons)


def create_biases(n_neurons):
    return np.random.randn(n_neurons)


class Layer():
    def __init__(self, n_inputs, n_neurons):
        self.weights = np.random.randn(n_inputs, n_neurons)
        self.biases = np.random.randn(n_neurons)

    def layer_forward(self, inputs):
        self.output = np.dot(inputs, self.weights) + self.biases
        return self.output


class Network():
    def __init__(self, network_shape):
        self.shape = network_shape
        self.n_layers = len(network_shape) - 1
        self.layers = []
        for i in range(self.n_layers):
            layer = Layer(network_shape[i], network_shape[i + 1])
            self.layers.append(layer)

    def network_forward(self, inputs):
        outputs = [inputs]
        for i in range(self.n_layers):
            layer_sum = self.layers[i].layer_forward(outputs[i])
            if i != self.n_layers - 1:
                layer_output = activation_ReLu(layer_sum)
                layer_output = normalize(layer_sum)
            else:
                layer_output = activation_softmax(layer_sum)
            outputs.append(layer_output)

        return outputs


def main():
    data = cp.create_data(5)
    cp.plot_data(data, 'original classification')
    print(data)
    inputs = data[:, (0, 1)]
    targets = copy.deepcopy(data[:, 2])
    print(inputs)
    network = Network(NETWORK_SHAPE)  # 建立
    outputs = network.network_forward(inputs)
    print(outputs)
    classification = classify(outputs[-1])
    print(classification)
    data[:, 2] = classification
    cp.plot_data(data, 'Before training')
    loss = precise_loss_function(outputs[-1], targets)
    print(loss)
    demand = get_final_layer_preAct_demands(outputs[-1], targets)
    print(demand)



main()
添加了需求函数，
就是拿输出的概率和正确答案做比较，进而找到优化方向





```
